{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8886ebfc-5708-465c-9925-ac7a9a22576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f285893-aba5-40ff-879e-0c69118f2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 API 키 설정\n",
    "client_id = 'YaKyDrFyIZDRfIiRAwBj'\n",
    "client_secret = 'hknmr5WHGm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef71359f-4a77-41e7-a8c0-7b29e74ae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Fine-tuned 모델 로드\n",
    "def load_sentiment_model():\n",
    "    model_name = \"beomi/kcbert-base\"  # KoBERT Fine-tuned 모델\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4626ffa2-611b-4565-a3e5-3a390ad81289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 수집 함수\n",
    "def get_news_data(client_id, client_secret, query, display=10, start=1, sort='date'):\n",
    "    enc_text = urllib.parse.quote(query)\n",
    "    url = f\"https://openapi.naver.com/v1/search/news?query={enc_text}&display={display}&start={start}&sort={sort}\"\n",
    "    \n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    \n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    \n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        response_json = json.loads(response_body)\n",
    "        return pd.DataFrame(response_json['items'])\n",
    "    else:\n",
    "        print(f\"Error Code: {rescode}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd484eac-2287-40b2-a529-45d41ba1766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 필터링 함수\n",
    "def filter_by_date(news_data, year):\n",
    "    news_data['pubDate'] = pd.to_datetime(news_data['pubDate'], errors='coerce')  # 날짜 형식으로 변환\n",
    "    news_data = news_data.dropna(subset=['pubDate'])  # 유효한 날짜가 없는 데이터 제거\n",
    "    news_data = news_data[news_data['pubDate'].dt.year == year]  # 특정 연도 필터링\n",
    "    news_data['Date'] = news_data['pubDate'].dt.strftime('%Y-%m-%d')  # YYYY-MM-DD 형식 생성\n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd49c556-3801-4909-9d3d-6a0209f8fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, tokenizer, model):\n",
    "    text = text.replace('<b>', '').replace('</b>', '').strip()  # HTML 태그 제거\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=300,  # 입력 텍스트를 300 토큰으로 제한\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        sentiment = torch.argmax(logits).item()\n",
    "        sentiment_map = {0: \"부정\", 1: \"중립\", 2: \"긍정\"}\n",
    "        return sentiment_map[sentiment]\n",
    "    except RuntimeError as e:\n",
    "        print(f\"RuntimeError 발생: {e}\")\n",
    "        return \"오류\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ad1efd4-2eef-48be-a798-4dddd5d2d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수\n",
    "def plot_sentiment_distribution(sentiment_count, query):\n",
    "    sentiment_count.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='coolwarm')\n",
    "    plt.title(f\"뉴스 검색어 '{query}' 날짜별 감정 분석 결과\")\n",
    "    plt.xlabel(\"날짜\")\n",
    "    plt.ylabel(\"감정 건수\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1881d0e5-cab1-4cde-a31b-d5798fe9f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 함수\n",
    "def main():\n",
    "    query = \"비트코인\"\n",
    "    display = 100  # 한 번에 가져올 뉴스 데이터 수\n",
    "    num_requests = 3  # 가져올 페이지 수 (100개씩)\n",
    "    year = 2024  # 필터링할 연도\n",
    "    sort = \"date\"  # 날짜순 정렬\n",
    "\n",
    "    # KoBERT 모델 로드\n",
    "    tokenizer, model = load_sentiment_model()\n",
    "\n",
    "    # 뉴스 데이터 수집\n",
    "    result_all = pd.DataFrame()\n",
    "    for i in range(num_requests):\n",
    "        start = 1 + display * i\n",
    "        result = get_news_data(client_id, client_secret, query, display, start, sort)\n",
    "        result_all = pd.concat([result_all, result])\n",
    "\n",
    "    # 날짜 필터링\n",
    "    result_all = filter_by_date(result_all, year)\n",
    "\n",
    "    if result_all.empty:\n",
    "        print(f\"{year}년의 뉴스 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 감정 분석 수행\n",
    "    result_all['Sentiment'] = result_all['description'].apply(\n",
    "        lambda x: analyze_sentiment(x, tokenizer, model)\n",
    "    )\n",
    "\n",
    "    # 날짜 및 감정별 집계\n",
    "    sentiment_count = result_all.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(sentiment_count)\n",
    "\n",
    "    # 그래프 생성\n",
    "    plot_sentiment_distribution(sentiment_count, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc2a67-64ae-4158-9308-4edd5957dd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ee111-0554-4796-8f9a-d1560df7757f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
